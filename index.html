<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="profile.jpg" alt="alt text" width="140px" height="HEIGHTpx" />&nbsp;</td>
<td align="left"><h2>Nazreen Shah Ambalath (S A Nazreen)<br /></h2>
  <p>PhD student,<br />
    <a href="https://www.iiitd.ac.in/">IIIT Delhi</a><br />
    B613, Intellicom Lab, RnD Block<br />
    email: <a href="mailto:nazreens@iiitd.ac.in">nazreens@iiitd.ac.in</a></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am a second year PhD student at IIIT, Delhi
  advised by <a href="https://www.iiitd.ac.in/ranjitha">Dr. Ranjitha Prasad</a> from September 2021. I am interested in Federated Learning and my research area is to attain personalization in Federated Learning. I am also interested in Federated Optimization.</p>

I was selected for the Prime Minister's Fellowship for Doctoral Research in July 2023.

<p>I have prior research experiences in ISI, Kolkata and IDRBT, Hyderabad, where I worked on image processing and reversible data hiding problems. My M.Tech thesis  <a href="https://drive.google.com/file/d/1Y50eKlyR5Zi-CkvE_xAugFDrZ-vov21W/view?usp=share_link">'Reversible Data Hiding in Color Images'</a> was completed under the guidance of <a href="https://www.idrbt.ac.in/dr-rajarshi-pal/">Dr Rajarshi Pal, IDRBT, Hyderabad.</a></p>
<p>I completed my 5-year integrated M.Tech in Computer Science from <a href="https://uohyd.ac.in/">University of Hyderabad (2020).</a></p>

<a href="CV.pdf">[Curriculum Vitae]</a> <a href="https://scholar.google.com/citations?user=7HCUafUAAAAJ&hl=en">[Google Scholar]</a> <a href="https://www.linkedin.com/in/nazreenshah1997/">[Linkedin]</a><a href="https://github.com/nazreenshah">[GitHub]</a><a href="https://wandb.ai/nazreen">[WANDB]</a></br>
</br>
<!-- <b><em>I am currently looking for research internships for 2023.</em></b> -->

<h2>Arxiv Pre-prints</h2>
<ul>
  <li><p><a href="https://arxiv.org/abs/2211.03363"><b>Over-The-Air Clustered Wireless Federated Learning</b></a> <br/> Ayush Madhan-Sohini, Divin Dominic, <em>Nazreen Shah</em>, Ranjitha Prasad</p>
  </li>
</ul>
<!-- <ul>
  <li><p><a href="FEDOSGD.pdf"><b>Distributed Online and Bandit Convex Optimization</b></a> <br /> <em>Kumar Kshitij Patel</em>, Aadrirupa Saha, Lingxiao Wang, Nathan Srebro<br />OPT ML Workshop, NeurIPS 2022</p>
  </li>
</ul>
<ul> -->

<!-- <h2>Publications</h2>
<ul>
  <li><p><a href="CMC.pdf"><b>On Convexity and Linear Mode Connectivity in Neural Networks</b></a> <br /> David Yunis, <em>Kumar Kshitij Patel</em>, Pedro Savarese, Karen Livescu, Matthew Walter, Jonathan Frankle, Michael Maire<br />OPT ML Workshop, NeurIPS 2022</p>
  </li>
</ul>
<ul>
  <li><p><a href="FEDOSGD.pdf"><b>Distributed Online and Bandit Convex Optimization</b></a> <br /> <em>Kumar Kshitij Patel</em>, Aadrirupa Saha, Lingxiao Wang, Nathan Srebro<br />OPT ML Workshop, NeurIPS 2022</p>
  </li>
</ul>
<ul>
  <li><p><a href="CELSGD.pdf"><b>Towards Optimal communication complexity in Distributed Non-convex Optimization</b></a> <a href="https://qrco.de/bdWwG5">[Recorded Talk]</a><br /> <em>Kumar Kshitij Patel</em>*, Lingxiao Wang*, Blake Woodworth, Brian Bullins, Nathan Srebro (*Equal Contribution)<br /> NeurIPS 2022</p>
  </li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.02954"><b>A Stochastic Newton Algorithm for Distributed Convex Optimization</b></a> <a href="https://www.youtube.com/watch?v=-AS5NWOaNqk">[Recorded Talk]</a><br /> Brian Bullins, <em>Kumar Kshitij Patel</em>, Ohad Shamir, Nathan Srebro, Blake Woodworth (Alphabetical ordering)<br /> NeurIPS 2021</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.04735"><b>Minibatch vs Local SGD for Heterogeneous Distributed Learning</b></a> <a href="https://videos.neurips.cc/search/minibatch%20vs/video/slideslive-38937666">[Recorded Talk]</a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Nathan Srebro <br /> NeurIPS 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.07839"><b>Is Local SGD Better than Minibatch SGD?</b></a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan McMahan, Ohad Shamir, Nathan Srebro <br /> ICML 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1808.07217#:~:text=Mini%2Dbatch%20stochastic%20gradient%20methods,scalability%20gains%20in%20recent%20years."><b>Don't Use Large Mini-batches, Use Local SGD</b></a><a href="https://github.com/epfml/LocalSGD-Code"> [Code]</a> <br /> Tao Lin, Sebastian U. Stich, <em>Kumar Kshitij Patel</em>, Martin Jaggi <br /> ICLR 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/file/4aadd661908b181d059a117f02fbc9ec-Paper.pdf"><b>Communication Trade-offs for Local-SGD with Large Step Size</b></a> <br /> <em>Kumar Kshitij Patel, Aymeric Dieuleveut</em> <br /> NeurIPS 2019</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-018-5758-5"><b>Corruption-Tolerant Bandit Learning</b></a> <br /> Sayash Kapoor, <em>Kumar Kshitij Patel, Purushottam Kar</em> <br /> Springer Machine Learning Journal 2019</p>
</li>
</ul>

<h2>Other Research Activities</h2>
<ul>
<li><p>I served/am serving as a reviewer for STOC'21, TMLR, ICML'21'22, NeurIPS'21'22, ICLR'22'23, AISTATS'22'23, Springer MLJ, as a session chair for ICML'22, NeurIPS'22, and as a volunteer for ICML'20, ICLR'20. I received the top reviewer award at ICLR'22, ICML'22, NeurIPS'22.</p>
</li>
<li><p>I am participating in the NSF-Simon's research collaboration on the <a href="https://www.simonsfoundation.org/grant/nsf-simons-research-collaborations-on-the-mathematical-and-scientific-foundations-of-deep-learning/">Mathematics of Deep Learning (MoDL)</a>.</p>
</li>
<li><p>I co-organized the <a href="https://ttic-student-workshop.github.io/">TTIC Student Workshop 2021</a>, with <a href="https://gxli97.github.io/">Gene Li</a>. We also organized a TTIC/Uchicago student theory seminar in Spring 2021. If you'd like to take over and re-start this series, please let me know.</p>
</li>
<li><p>I was a Teaching Assistant for the Convex Optmization course at TTIC and a co-organizer for the <a href="https://www.ttic.edu/research-at-ttic/">Research at TTIC Colloquium</a> for Fall-Winter 2021.</p>
</li>
<li><p>I participated in the <a href="http://mlss.tuebingen.mpg.de/2020/">Machine Learning Summer School at T&uumlbingen, Germany</a> during summer 2020.</p></li>
</ul> -->

</div>
</body>
</html>
